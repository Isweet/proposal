\chapter{\mpc, A Language for Concise MPC}
\label{ch:mpc}

The purpose of this chapter is to provide a summary of prior work on \mpc.~\Cref{sec:mpc-theory} describes \mpc
formally, defining the syntax, sequential semantics, and distributed semantics. These definitions are used to
prove~\nameref{thm:mpc-simulation}.~\Cref{sec:mpc-impl} describes \system, a Haskell interpreter for \mpc.
\system includes both the sequential and distributed semantics, as well as an MPC backend based on EMP~\cite{}.
It also has additional features not included in the formal model of \mpc. Finally, a set of experiments on a
benchmark suite suggest that performance of \system is similar to Obliv-C. Interested readers can find
additional information on \mpc and \system in~\citet{todo}.

\section{Design}
\label{sec:mpc-design}

In addition to the definition of the syntax (\Cref{fig:mpc-syntax}), the formal definition of \mpc
includes two dynamic semantics --- sequential (\Cref{fig:mpc-seq-defs,fig:mpc-seq-sem}) and distributed (\Cref{fig:mpc-dist-sem}).
The sequential semantics represents the local execution of the program, requiring no communication
and no MPC. The distributed semantics represents the distributed execution, requiring communication
between principals performing MPC. The~\nameref{thm:mpc-simulation} theorem ensures that successfully
terminating programs produce the same answer, whether they are executed locally (sequential semantics),
or deployed as a distributed system performing MPC (distributed semantics).

The syntax of \mpc is in \Cref{fig:mpc-syntax}. To simplify the formal semantics,
\mpc is defined using a kind of \emph{administrative normal form} (ANF), meaning
that most expression forms operate directly on variables $x$, rather than subexpressions $e$,
as is the case in the actual implementation.
\footnote{ This restriction does not harm expressiveness. Direct-style syntax like ⸨⦑ref⦒␣(1+⦑read⦒)⸩
can be encoded in \mpc's formal syntax as ⸨⦑let⦒␣x = ⦑read⦒␣⦑in⦒␣⦑let⦒␣y = 1␣⦑in⦒␣⦑let⦒␣z = y+x␣⦑in⦒␣⦑ref⦒␣z⸩. }

\FigureMPCSyntax{fig:mpc-syntax}{\mpc formal syntax.}

\subsection{Standard Expressions}
\label{subsec:mpc-design-standard}

Most of \mpc's expressions are standard. We isolate \emph{atomic expressions} $a$
as a sub-category of full expressions $e$. They include
variables $x$;
integers $i$;
party sets $p$;
binary operations ⸨x ⊙ y⸩ (left abstract, but could be, e.g., addition and multiplication);
multiplexor ⸨x ¿ y ◇ z⸩ (an “if” expression which evaluates both branches and returns one result);
sum injection ⸨ιᵢ␣x⸩ (i.e., tagged union creation);
pair creation ⸨⟨x,y⟩⸩ and projection ⸨πᵢ␣x⸩;
recursive function creation ⸨λ⸤z⸥x⍪ e⸩;
and reference creation ⸨⦑ref⦒␣x⸩ dereference ⸨¡x⸩ and assignment ⸨x ≔ y⸩.
Sum elimination ⸨⦑case⦒␣x␣❴y⍪e⸤1⸥❵␣❴y⍪e⸤2⸥❵⸩ is a full expression ⸨e⸩
because it does not reduce to a value in one step; here, $y$ binds in
the branch bodies $e_i$, only one of which is evaluated.
\footnote{ We can encode ⸨⦑if⦒␣x␣⦑then⦒␣e⸤1⸥␣⦑else⦒␣e⸤2⸥⸩ as
⸨⦑let⦒␣z=0␣⦑in⦒␣⦑let⦒␣y=x ¿ ι₁␣z ◇ ι₂␣z␣⦑in⦒␣⦑case⦒␣y␣❴\_⍪e⸤1⸥❵␣❴\_⍪e⸤2⸥❵⸩. }
Function application ⸨x␣y⸩ and local variable binding ⸨⦑let⦒␣x=e⸤1⸥␣⦑in⦒␣e⸤2⸥⸩
describe full expressions for the same reason. The ⸨⦑par⦒[p]␣e⸩ expression
is specific to the distributed programming setting, which we describe next.

\subsection{Distributed Computing Expressions}
\label{subsec:mpc-design-par}

The \mpc expression ⸨⦑par⦒[x]␣e⸩ says that if $p$ is the
party set contained in variable $x$, then $e$ may be computed at
parties $p$ in parallel (hence the syntax ⸨⦑par⦒⸩). That is, every
party $A \in p$ may evaluate $e$. We say ``may'' here because
nesting such an expression in another ⸨⦑par⦒⸩ could shrink the set of
parties. For example, $e$ in ⸨⦑par⦒[x]␣(⦑par⦒[y]␣e)⸩ will be
evaluated by $p \cap q$ when $y$ is party set $q$; if this
intersection is $\eset$ then $e$ is essentially dead code. The fact that
party sets are \emph{first class} (i.e., can be stored in
variables) and not simply literal annotations allows \mpc
to support rich coordination patterns.

We call the set of parties $m$ computing an expression in parallel
the \emph{mode}. We say the parties $A \in m$ are \emph{present}
for a computation. The semantics of many constructs depends on the
mode. A number $i$ created in mode $m$ is
known only to parties $A \in m$ and notated ⸨i@m⸩. This means that adding two numbers
located at $p$ can only be done in a mode $m$ such that
$m \subseteq p$; if the mode contained additional parties
$A \not\in p$ then they wouldn't know what to do; such states will be
stuck in our semantics. The same goes for functions, sums, and
references. The ⸨⦑read⦒⸩ and ⸨⦑write⦒␣x⸩ expressions perform local
I/O and so can only be run in a mode with a single party.

On the other hand, it is possible that a variable $x$ is in scope for
$A$, but maps to a value only usable by $B$. Party $A$ can
still manipulate a placeholder for $x$ (e.g., to store it in a
data structure or pass it to a function) but may never compute with its
contents (e.g., add to it or branch on it). This approach
simplifies the design of the language at the cost of missing some
(unlikely, but ultimately harmless) logic errors.

\subsection{MPC Expressions}
\label{subsec:mpc-design-mpc}

The \mpc expression ⸨⦑share⦒[x→y]␣z⸩ directs $p$ ($x$'s contents),
required to be a singleton party set, to create secret shares of $z$,
an integer, and distribute the shares to each party in
$q$ ($y$'s contents). All parties $p \cup q$ must be present.
Parties $q$ can all mutually compute on a shared, encrypted value
using ⸨x {⊙} y⸩ and ⸨x {¿} y {◇} z⸩.  Recall from above that the latter is a
multiplexor: we select between ⸨y⸩ and ⸨z⸩ based on whether $x$ is zero or
non-zero. The former models binary operations over numeric types.
An MPC is completed by invoking ⸨⦑reveal⦒[x→y]␣z⸩. This takes $z$, a share
among a set of parties $p$ (the contents of $x$) and converts it to cleartext,
sharing the result among parties $q$ (the contents of $y$). Doing so requires that all of
$p \cup q$ are present so that the shareholders can agree to send the value,
and the result-receivers are ready to receive it.

\subsection{Sequential Semantics}
\label{subsec:mpc-design-seq}

This section defines a small-step, \emph{sequential} (ST)
operational semantics for \mpc. Its execution model treats all
participating parties as if they were executing in lockstep.
We prove that the ST semantics faithfully models the \emph{distributed} (DS)
semantics presented in \Cref{subsec:mpc-design-dist}, according to which parties may act
independently. Thus, the ST semantics can serve as the basis of \mpc formal reasoning,
e.g., about correctness and security.

\FigureMPCSemanticsAux{fig:mpc-aux}{\mpc definitions and metafunctions used in formal semantics.}

\subsubsection{Located Values}
\label{subsubsec:mpc-design-seq-val}

When evaluated, a \mpc expression either loops or returns a \emph{value},
of which their are two kinds. \emph{Located values} ⸨u⸩, defined in
\Cref{fig:mpc-aux}, are only accessible to particular parties. The form
⸨u@m⸩ is a \emph{value}, indicating that the located value ⸨u⸩ is
accessible to each party ⸨A ∈ m⸩, e.g., because it was the result of
evaluating ⸨⦑par⦒[x]␣e⸩ when ⸨x⸩ contained party-set $m$. Values for numbers ⸨i⸢ψ⸣⸩, sets of parties
⸨p⸩, sums ⸨ιᵢ␣v⸩, pairs ⸨⟨v,v⟩⸩, recursive closures ⸨⟨λ⸤z⸥x⍪e,γ⟩⸩ (which include
a closure environment ⸨γ⸩ and “self” binder ⸨z⸩), and memory locations (i.e.,
pointers) ⸨ℓ⸢⋕p⸣⸩ are all located, and otherwise standard except for annotations
⸨ψ⸩ and ⸨⋕p⸩.

For integer values ⸨i⸢ψ⸣⸩, we write just ⸨i⸩ when the annotation ⸨ψ⸩ is~⸨⋅⸩
(i.e., ⸨ψ⸩ is not written at all), and this represents a cleartext value. When
the annotation ⸨ψ⸩ is ⸨⦑enc⦒⋕p⸩ this represents an encrypted value shared among
parties ⸨‹B› ∈ p⸩ (a ``share''). Thus, a value ⸨1⸢⦑enc⦒⋕q⸣@q⸩ can be read as “an
integer 1, encrypted (i.e., secret shared) between parties ⸨q⸩, and accessible to parties
⸨q⸩.” The duplication of ⸨q⸩ may seem redundant, but they may differ
in other contexts. The first ⸨q⸩ represents \emph{among whom is this value shared}
(determined when the share is created), and the second ⸨q⸩ represents
\emph{who has access to this value} (determined by the enclosing ⦑par⦒ blocks).
Location annotations are only used in the sequential semantics in order to
simulate the presence of multiple parties; they are unused in the distributed
semantics and final execution. On the other hand, the ⸨⦑enc⦒⋕q⸩ annotation is
used in distributed execution to detect at runtime buggy programs which fail to
coordinate properly, e.g., if ⸨‹A›⸩ attempts to do arithmetic on a secret share
owned by both ⸨‹A›⸩ and ⸨‹B›⸩, but while ⸨‹B›⸩ is not present.

Memory locations ⸨ℓ⸩ are annotated with ⸨⋕p⸩ to indicate the parties
⸨p⸩ that are \emph{co-creators} of the referenced memory. \emph{Values}
are either ⸨u@m⸩ indicating the located value ⸨u⸩ is only accessible to ⸨C ∈ m⸩,
or they are the \emph{opaque value} ⸨★⸩ which indicates the value is both
unknown and inaccessible.

The same figure defines the function ⸨‗↙ₘ⸩, which is used to \emph{(re)locate} a
value to a scope with parties ⸨m⸩ present. For located values ⸨u@p⸩, ⸨‗↙ₘ⸩
relocates them to ⸨p ∩ m⸩, unless the intersection is empty in which case the
value is inaccessible, so it becomes ⸨★⸩. Relocating is a deep operation;
⸨u@p↙ₘ⸩ also relocates the contents ⸨u⸩ to ⸨u↙ₘ⸩, which recurses over
the sub-terms of ⸨u⸩. This step has no effect on closures, though: the closure's
environment's variables get relocated when they are referenced, as we will see.

\FigureMPCSemanticsSequential{fig:mpc-seq}{\mpc sequential semantics.}

\subsubsection{Semantics}
\label{subsubsec:mpc-design-seq-sem}

The small-step, sequential semantics is given in \Cref{fig:mpc-seq}. The
main judgment ⸨ς —→ ς⸩, shown at the bottom, is expressed as rules that step
between \emph{configurations} ⸨ς⸩, which are 5-tuples comprising the current
mode ⸨m⸩, environment ⸨γ⸩, store ⸨δ⸩, stack ⸨κ⸩, and expression ⸨e⸩.
Per~\Cref{fig:mpc-aux}, environments are partial maps from variables to
values, and stores are partial maps from memory locations to values. The main
judgment refers to judgment ⸨γ ⊢ₘ δ,a ↪ δ,v⸩, also shown in the figure, which
defines the evaluation of atomic expressions ⸨a⸩ to values ⸨v⸩.

A stack ⸨κ⸩ is defined by the grammar at the top of
\Cref{fig:mpc-seq}; it is either the empty stack ⸨⊤⸩ or a
list of frames ⸨⟨⦑let⦒␣x=□␣⦑in⦒␣e¦m,γ⟩∷κ⸩. Rule~*⦗ST-LetPush⦘ evaluates
⸨⦑let⦒␣x=e₁␣⦑in⦒␣e₂⸩ by pushing a frame (which includes the skeleton of the
⸨⦑let⦒⸩, the current mode ⸨m⸩, and the environment ⸨γ⸩) and then evaluating
⸨e₁⸩. By rule~*⦗ST-LetPop⦘, when the current expression is atomic, we
evaluate it to a value ⸨v⸩, and then pop the top frame and evaluate its
contents ⸨e₂⸩ in the frame's mode ⸨m⸩ and in its captured environment ⸨γ′⸩
extended with variable ⸨x⸩ mapped to value ⸨v⸩.

The rules satisfy two key invariants. First, if ⸨γ ⊢ₘ δ,a ↪ δ,v⸩ then ⸨v⸩ is «compatible
with» ⸨m⸩. A value ⸨v⸩ is compatible with a set of parties ⸨m⸩ when it is
accessible to some set of parties ⸨p ⊆ m⸩. Formally, the compatibility of ⸨v⸩
with ⸨m⸩ is captured by ⸨v↙ₘ = v⸩; that is, ⸨v⸩ is already located at ⸨m⸩, and
re-locating it there does nothing. (Note that ⸨‗↙ₘ⸩ is idempotent.)
The second invariant is that to \emph{destruct} a value located at ⸨m⸩
requires running in mode ⸨m⸩, i.e., all parties which know the value must be
present. This invariant helps ensure these parties, when running in a
distributed setting with their own store, environment, etc. will agree on the
result.

\paragraph*{Variables, Literals, Binding, and Closures}

Rule~*⦗ST-Var⦘ retrieves a variable's value from the environment and
locates it to the current mode ⸨m⸩ via ⸨γ(x)↙ₘ⸩. Rule~*⦗ST-Lit⦘ evaluates
cleartext constant ⸨i⸩ to the value ⸨i@m⸩, or party set ⸨p⸩ to the value ⸨p@m⸩;
the location is based on the evaluation mode ⸨m⸩.  Rule~*⦗ST-PSetBinop⦘ computes
the union of two party sets in the expected way. Rules~*⦗ST-IntBinop⦘
and~*⦗ST-Mux⦘ capture arithmetic and atomic conditional functions over integers,
but in a way that is parametric in cleartext and encrypted (via multiparty
computation) modalities; we say more, shortly.

Because locations are narrowed when a variable is accessed, we don't need to
explicitly (re)locate a closure's environment ⸨γ⸩ in the relocating
operation in Rule~*⦗ST-Fun⦘. Rule~*⦗ST-App⦘ requires the closure to be available at ⸨m⸩,
the current mode, and sets of evaluation of its body in the standard way.

\paragraph*{Par mode}

The current mode ⸨m⸩ may change due to ⸨⦑par⦒[x]␣e⸩, which denotes a
sub-computation ⸨e⸩ to be evaluated only by parties in the set stored
in ⸨x⸩; call that set ⸨p⸩. Operationally, ⸨⦑par⦒[x]␣e⸩ evaluates ⸨e⸩ in mode
⸨m ∩ p⸩; i.e., only those parties in ⸨p⸩ which are «also» present in ⸨m⸩ will run ⸨e⸩.
When ⸨m ∩ p⸩ is non-empty, rule~*⦗ST-Par⦘ directs ⸨e⸩ to evaluate in the refined mode.
If ⸨m ∩ p⸩ is empty, then per rule~*⦗ST-ParEmpty⦘, ⸨e⸩ is skipped and ⸨★⸩ is returned.
(Since ⸨★⸩ is a value, not an expression, we return a fresh variable and the
environment with that variable mapped to ⸨★⸩.) Note that because the stack
tracks each frame's mode, when the current expression completes the old
mode will be restored when a stack frame is popped.

\paragraph*{Sums, Pairs, and Party Sets}

Sums and pairs are essentially standard, modulo the consideration of their values' locations.
%
Rule~*⦗ST-Inj⦘ introduces a sum, and rule~*⦗ST-Case⦘ eliminates it, binding the
contents to ⸨x⸩ in the appropriate branch ⸨i ∈ ❴1,2❵⸩, in the usual way.
%
Rule~*⦗ST-Pair⦘ introduces a pair, and rule~*⦗ST-Proj⦘ eliminates it.
%
Party sets can be constructed by set-union in rule~*⦗ST-PSetBinop⦘,
and eliminated in the style of sums using rules~*⦗ST-Case-PSet-Emp⦘
and~*⦗ST-Case-PSet-Cons⦘. The former rule eliminates an empty set,
evaluating to first-branch expression ⸨e₁⸩; the latter rule binds an
arbitrarily chosen singleton principal ⸨❴A❵⸩ from ⸨x₁⸩ to ⸨x₂⸩ and the
remainder ⸨p⸩ to ⸨x₃⸩ when evaluating second-branch ⸨e₂⸩.

\paragraph*{References}

Rule~*⦗ST-Ref⦘ creates a fresh reference in the usual way, returning a located
pointer, but annotated with the parties that created it.
Rule~*⦗ST-Deref⦘ takes a reference located in the current mode ⸨m⸩ and
returns the pointed-to contents made compatible with ⸨m⸩. Rule
*⦗ST-Assign⦘ updates the
store with the new value and returns it, as usual, but only works for
⸨ℓ⸢⋕p⸣⸩ references where ⸨p = m⸩, the current mode. Why? Consider the
following example.

M⁅ Aːl@{␣}l
   A⁅ ⦑par⦒[A,B] & ⦑let⦒␣x=⦑ref⦒␣0␣⦑in⦒
   A⁃            & ⦑let⦒␣‗= (⦑par⦒[A]␣x ≔ 1)␣⦑in⦒
   A⁃            & ⦑let⦒␣y = ¡x␣⦑in⦒␣…
   A⁆
M⁆

The variable ⸨x⸩ initially contains a reference ⸨ℓ⸢⋕❴A,B❵⸣⸩ because it was
created in a context with mode ⸨m=❴A,B❵⸩. Then ⸨x⸩ is assigned to by
⸨‹A›⸩ in the ⸨⦑par⦒⸩ expression on the subsequent line. By
rule~*⦗ST-Assign⦘, the creators of the reference ⸨⋕❴A,B❵⸩
must match mode $m$ to proceed, but since $m$ is ⸨❴A❵⸩ the program is stuck.
This is desirable because to proceed would cause the ⸨‹A›⸩'s and ⸨‹B›⸩'s views
of the computation to get out of sync. When we run this program at each of ⸨‹A›⸩ and
⸨‹B›⸩ separately, as part of the distributed semantics, on ⸨‹A›⸩ we would do the assignment but on
⸨‹B›⸩ it would be skipped. As such, on ⸨‹A›⸩ the value of ⸨y⸩ would be ⸨1⸩ but on
⸨‹B›⸩ it would be ⸨0⸩. If the ⸨…⸩ part of the program were to branch on ⸨y⸩ and
then in one branch do some MPC constructs but not in the other, then the two
parties would fall out of sync.

\paragraph*{I/O}

Rules~*⦗ST-Read⦘ and~*⦗ST-Write⦘ handle I/O. They require that the mode is a
singleton party, and locate the resulting value at the singleton set for that party.
In the semantics, ⦑read⦒ nondeterministically returns any integer,
which over-approximates the behavior of reading a particular integer as input
from the host environment.

\paragraph*{MPC}

The remaining rules cover constructs for MPC, as well as local,
integer-based computations.
%
Rule~*⦗ST-Binop⦘ handles arithmetic integer operations which are supported in
both plaintext and encrypted modes, such as ⸨+⸩ and ⸨×⸩. The protocol
⸨ψ⸩ must be the same for both integer arguments, and it must be compatible with the
current mode ⸨m⸩, per the judgment ⸨⊢ₘ ψ⸩. This judgment (not shown) holds when ⸨ψ⸩ is
either~⸨⋅⸩ or ⸨⦑enc⦒⋕m⸩, that is, both integers are either cleartext or
encrypted values shared among those present in ⸨m⸩. The result depends on the
particular operation ⸨⊙⸩, as determined by the semantic function ⸨⟦‗⟧ ∈ ‹binop›
→ ℤ × ℤ → ℤ ⸩ which we leave opaque.
%\footnote{We assume that other operations can be added,
%  depending on the underlying cryptographic scheme. In GMW,
%  comparisons such as $<$ can be encoded using arithmetic operations.}
Rule~*⦗ST-Mux⦘ is similar to *⦗ST-Binop⦘ and functions as a ternary atomic
conditional; it handles a multiplexor according to the semantic function
⸨‹cond›⸩ where ⸨‹cond›(i₁,i₂,i₃) ≜ i₂⸩ when ⸨i₁ ≠ 0⸩, and ⸨‹cond›(i₁,i₂,i₃) ≜ i₃⸩
when ⸨i₁ = 0⸩.

Party ⸨‹A›⸩ can create an encrypted value (i.e., a
secret-share) shared among parties ⸨q⸩ using ⸨⦑share⦒[x₁ → x₂]␣x₃⸩ handled by
rule~*⦗ST-Share⦘. Variable ⸨x₁⸩ must be a singleton set of the input party
⸨p⸩ where ⸨|p|=1⸩; variable ⸨x₂⸩ must be the (nonempty) set of parties
who wild hold shares ⸨q ≠ ∅⸩; and
⸨x₃⸩ must be the input share value known to a superset of input parties ⸨p′ ⊇ p⸩.
The input party (singleton) set ⸨p⸩ and share parties ⸨q⸩ must all be present in
the mode ⸨m⸩, and no other parties may be present (so ⸨m=p∪q⸩). The resulting
value is located at ⸨q⸩, and has protocol ⸨⦑enc⦒⋕q⸩ indicating it is an
encrypted value shared among parties ⸨q⸩.

The constraint ⸨p ⊆ q⸩ highlighted in blue in rule~*⦗ST-Share⦘
is required for what we call «lazy» MPC protocols. Lazy protocols are those that
don't execute cryptographic operations when evaluating primitive operations
(such as ⸨+⸩ and ⸨×⸩), instead they build circuits which are later executed at
the point of a ⸨⦑reveal⦒⸩ expression. The constraint ⸨p ⊆ q⸩ ensures that all
input parties will be present during the reveal, as they will need to provide
their secret inputs at that time. Yao-based backends are often lazy
in this sense. Backends such as GMW are typically strict in that circuits are
never constructed for later execution, rather secret values are immediately
“secret-shared” when encountering a ⸨⦑share⦒⸩ expression, and the cryptographic
protocol is performed directly for each primitive operation. Because input
parties need not be present for this strict execution, the ⸨p ⊆ q⸩ is not
necessary when evaluating with such backends.

A shared encrypted value is revealed from parties ⸨p⸩ to a nonempty set of
parties ⸨q ≠ ∅⸩ as a cleartext result via the ⸨⦑reveal⦒[x₁→x₂]␣x₃⸩, where ⸨x₁⸩
evaluates to ⸨p⸩ and ⸨x₂⸩ evaluates to ⸨q⸩, and ⸨x₃⸩ evaluates to the encrypted
value, as described by rule~*⦗ST-Reveal⦘. All parties ⸨p⸩ among which ⸨x₃⸩
is shared must be present, as well as the recipients of the value ⸨q⸩, and no
other parties. % See~\Cref{sec:overview} (e.g.,
% \Cref{fig:other-algs,fig:millionaires-MT}) for examples involving these
% constructs.

\section{Implementation}
\label{sec:mpc-impl}

\ins{TODO}




% OLD ----vvvv

The first exotic feature is \emph{located types}. These types are located
in the sense that they can be manipulated only at particular principals.
The metavariable ⸨m⸩ represents a set of parties, and a type is written ⸨σ@m⸩. The \mpc expression
⸨⦑par⦒[p]␣e⸩ determines the locatedness. It says that ⸨e⸩ may be computed at
parties ⸨p⸩ in parallel (hence the syntax ⸨⦑par⦒⸩). That is, every
party ⸨‹A› ∈ p⸩ may evaluate ⸨e⸩. We say ``may'' here because
nesting such an expression in another ⸨⦑par⦒⸩ could shrink the set of
parties. For example, ⸨e⸩ in ⸨⦑par⦒[p]␣(⦑par⦒[q]␣e)⸩ will be
evaluated by ⸨p ∩ q⸩; if this intersection is ⸨ø⸩ then ⸨e⸩
is essentially dead code.

We call the set of parties ⸨m⸩ computing an expression in parallel
the \emph{mode}. We say the parties ⸨‹A› ∈ m⸩ are \emph{present}
for a computation. The semantics of many constructs depends on the
mode. A number $i$ created in mode $m$ (having type ⸨⦑int⦒@m⸩) is
known only to parties $A \in m$. This means that adding two numbers
located at $p$ can only be done in a mode $m$ such that
$m \subseteq p$; if the mode contained additional parties
$A \not\in p$ then they wouldn't know what to do; such states will be
stuck in our semantics. The same goes for pairs and functions.
The ⸨⦑read⦒␣μ⸩ and ⸨⦑write⦒␣e⸩ expressions perform local
I/O and so can only be run in a mode with a single party.

On the other hand, it is possible that a variable $x$ is in scope for
$A$, but maps to a value only known to $B$. Party $A$ can
still manipulate a placeholder for $x$ (e.g., to store it in a
datastructure or pass it to a function) but may never compute with its
contents (e.g., add to it or branch on it). This approach
simplifies the design of the language at the cost of missing some
(unlikely, but ultimately harmless) logic errors.

Generally speaking, \mpc's design aims to ensure that any expression
$e$ of type $\sigma @ m$ will have the \emph{same run-time value} at
each $A \in m$. This is a key invariant underpinning \mpc's sequential
interpretation.

The second exotic feature is \emph{secret share types}.
Base types are annotated with a
\emph{protocol} $\psi$ that indicates whether they are either
cleartext ($\cdot$) or \emph{encrypted} and shared among parties
$p$ (⸨⦑enc⦒⋕p⸩). For concreteness, we imagine the encryption protocol is
Goldreich, Widgerson, and Micali (GMW)~\citeyear{STOC:GolMicWig87},
and values of this type are \emph{secret shares}, but the formal
semantics is agnostic to the cryptographic details. Note that we
often just write ⸨⦑int⦒⸩ for cleartext integer types, to reduce
clutter (i.e., eliding the $\cdot$).

The \mpc expression ⸨⦑share⦒[p→q]␣e⸩ directs $p$ (required to be a singleton party set) to
create secret shares of $e$, an integer, and distribute the shares to
each party in
$q$. All parties $p \cup q$ must be present. The resulting value
has type ⸨⦑int⦒⸢⦑enc⦒⋕q⸣@q⸩. This type reads “an integer, encrypted
(i.e., secret shared) between parties ⸨q⸩, and accessible to parties
⸨q⸩”. The duplication of ⸨q⸩ may seem redundant, but they may differ
in other contexts. The first ⸨q⸩ represents \emph{who has the
shares} (determined when the share is created), and the second ⸨q⸩
represents \emph{who has access to this value} (determined by the
enclosing ⦑par⦒ blocks). If this encrypted value flows to a part of
the program only executed by ⸨q′ ⊂ q⸩, then it will not be possible to
recombine shares, since not all parties ⸨q⸩ will be present.

Parties $q$ can all mutually compute on a shared, encrypted value
using ⸨e₁ ⊙ e₂⸩ and ⸨e ¿ e₁ ◇ e₂⸩. The latter is a
multiplexor: we select between ⸨v₁⸩ and ⸨v₂⸩, the results of evaluating ⸨e₁⸩ and ⸨e₂⸩,
based on whether $e$ is ⸨true⸩ or ⸨false⸩. The former models binary operations over numeric and boolean types.
When operating on encrypted values, both the multiplexor and some binary
operation expressions will necessitate \emph{communication} in the
distributed semantics, and an actual implementation will use an
underlying MPC protocol to implement the computation over the encrypted value.
All parties to which a share was sent must be present when computing on it. E.g., for numbers of type
⸨⦑int⦒⸢⦑enc⦒⋕{A,B}⸣⸩ to be added, both $A$ and $B$ must be
present. Indeed, it must be \emph{exactly} these parties which are
present; we do not allow more, since other parties would not be
able to carry out the operation (they don't have access to the
share).

An MPC is completed by invoking ⸨⦑reveal⦒[q]␣e⸩. This takes a share
(among some set of parties $p$) and converts it to cleartext,
sharing the result among parties $q$. Doing so requires that all of
$p \cup q$ are present so that the shareholders
can agree to send the value, and the result-receivers are ready to
receive it.

Locatedness is crucial in proving that \mpc{} satisfies~\nameref{thm:mpc-simulation}.
The~\nameref{thm:mpc-simulation} property is what allows us to view the language as \textbf{Abstractly Sequential}. The
next section is dedicated to explaining this property, and highlighting some of its consequences.

\subsection{Simulation}
\label{sec:lam-mpc-simulation}

\subsubsection{Sequential Semantics}

\ins{TOOD: put actual figure here?}

The sequential semantics takes the form of a judgement ⸨ς —→ ς⸩ which
defines when one configuration steps to another. A configuration
⸨ς ⩴ m,γ,κ,e⸩ is a 4-tuple comprising the current mode ⸨m⸩, environment ⸨γ⸩, stack
⸨κ⸩, and expression ⸨e⸩. Environments ⸨γ ⩴ var ⇀ value⸩, map variables to
values. A stack ⸨κ⸩ is a list of frames ⸨⟨⦑let⦒␣x=□␣⦑in⦒␣e¦m,γ⟩⸩, where
⸨⊤⸩ represents the empty stack. Values, ⸨v ⩴ u@m␣|␣★⸩, are either located
values ⸨u⸩ at mode ⸨m⸩ or else are opaque, ⸨★⸩. A located value, ⸨u⸩,
contains all standard values mutually defined on ⸨v⸩, e.g. a pair value is ⸨⟨v, v⟩⸩.
Base values are tagged with a protocol ⸨ψ⸩. For example, a
pair of shares among ⸨\{A,B\}⸩ is ⸨⟨i₁⸢⦑enc⦒⋕\{A,B\}⸣@\{A,B\},i₂⸢⦑enc⦒⋕\{A,B\}⸣@\{A,B\}⟩@\{A,B\}⸩.

Finally, the semantics heavily relies on a metafunction, ⸨‗↙ₘ ∈ ‹value› → ‹value›⸩, which \emph{(re)locates} a
value to a scope with parties ⸨m⸩ present. For located values ⸨u@p⸩, ⸨‗↙ₘ⸩
relocates them to ⸨p ∩ m⸩, unless the intersection is empty in which case the
value is inaccessible, so it becomes ⸨★⸩. Relocating is a deep operation;
⸨u@p↙ₘ⸩ also relocates the contents ⸨u⸩ to ⸨u↙ₘ⸩, which recurses over
the sub-terms of ⸨u⸩. This step has no effect on closures, though: the closure's
environment's variables get relocated when they are referenced.

The semantics ensures that whenever a located value ⸨u⸩ is introduced, it is tagged with
the appropriate mode ⸨m⸩ from the current configuration ⸨ς⸩. The relocation metafunction
is used on variable access, to relocate a value to the mode of the current lexical scope.
For example, the program ⸨⦑let⦒␣x = ⦑par⦒␣[A,B] 10␣⦑in⦒␣⦑par⦒␣[A]␣x⸩ should evaluate to
⸨10@{A}⸩. Likewise, when a value ⸨u@p⸩ is eliminated, we check that all necessary parties
are present ⸨m ⊆ p⸩. These two patterns ensure that the sequential semantics will get stuck
whenever a distributed deployment would get stuck. For example, in the program
⸨⦑par⦒␣[A,B]␣(⦑par⦒␣[A]␣10) + (⦑par⦒␣[B]␣20)⸩, the introduction rules for integers ensure that
the inner ⸨⦑par⦒⸩ expressions will reduce to ⸨10@\{A\}⸩ and ⸨20@\{B\}⸩. The elimination rule, ⸨+⸩,
will check ⸨\{A,B\} ⊆ \{A\}⸩ and ⸨\{A,B\} ⊆ \{B\}⸩ which will fail, causing the program to get stuck.

\subsubsection{Distributed Semantics}

The distributed semantics is defined by a judgement ⸨D ↝ D⸩ which says that
one distributed configuration, ⸨D ∈ ‹party› ⇀ ‹local-config›⸩, can step to another.
A configuration, ⸨D⸩, is a partial mapping between parties and their local
view of execution. A ⸨‹local-config›⸩ is just like a sequential configuration, ⸨ς⸩,
except that the components contain local values, ⸨⇡.v⸩. Local values are just like
sequential values, except that location annotations ⸨‗@m⸩ have been stripped away.
For example, a pair of shares among ⸨\{A,B\}⸩ is ⸨⟨i₁⸢⦑enc⦒⋕\{A,B\}⸣,i₂⸢⦑enc⦒⋕\{A,B\}⸣⟩⸩.

For most expressions, the distributed semantics simply steps an individual party independently of
the rest. To step an individual party, ⸨P⸩, we execute a step on its local configuration, ⸨D(P)⸩.
These individual steps look very similar to sequential steps, except that mode annotation is not added
or checked. A local configuration still has a mode component, because we do need to check that the mode
agrees with, for example, the protocol tag ⸨⦑enc⦒⋕q⸩. When locally stepping an expression ⸨⦑par⦒␣[P]␣…⸩
we additionally check that ⸨P ∈ p⸩ where ⸨P⸩ is the whose local configuration we are stepping.

The only expressions which do not step on an
invididual party are the \emph{synchronizing} expressions, ⸨⦑share⦒⸩ and ⸨⦑reveal⦒⸩. These expressions
step by transferring data from one party to the other(s), and so the rules maniuplate multiple local
configurations at once. In both the sequential and distributed rules we require that (1) the sharer,
⸨p⸩, be a singleton ⸨|p| = 1⸩, (2) the sharees, ⸨q⸩, be non-empty ⸨q ≠ ø⸩, and (3) that only the
sharer and sharees are present ⸨m = p ∪ q⸩. In distributed mode, we move the shared value from ⸨p⸩
to ⸨q⸩ by pulling ⸨⇡.v⸩ from ⸨D(p)⸩ and updating all the local configurations ⸨D(q)⸩.

\subsubsection{Correspondence}

The key observation that makes these semantics agree is that if the sequential semantics
gets stuck due to an elimination form with a failing mode check then the distributed semantics
will get stuck due to an elimination form attempting to inspect the opaque value ⸨★⸩. For example,
let's return to the example ⸨⦑par⦒␣[A,B]␣(⦑par⦒␣[A]␣10) + (⦑par⦒␣[B]␣20)⸩. We saw that this expression
gets stuck in the sequential semantics due to a failure of both mode checks when evaluating ⸨+⸩:
⸨\{A,B\} ⊈ \{A\}⸩ and ⸨\{A,B\} ⊈ \{B\}⸩. In the distributed semantics, we will execute each party
independently, eventually arriving at ⸨10 + ★⸩ on $A$ and ⸨★ + 20⸩ on $B$. Both parties will get
stuck, and the entire distributed configuration will be stuck.

We can state our formal correspondence between the sequential and distributed semantics formally
as~\nameref{thm:mpc-simulation}. This theorem says that for any well-typed sequential configuration,
⸨ς⸩, ⸨ς⸩ successfully terminates as ⸨ς'⸩ if and only if the corresponding distributed configuration
⸨ς↯⸩ successfully terminates as ⸨ς'↯⸩. The ⸨‗↯⸩ metafunction takes a sequential configuration to an
appropriate distributed configuration. We call it this metafunction \emph{slicing} since it ``slices''
away all of the mode annotations appropriately, leaving a distributed configuration in which each
party is mapped to a local configuration containing only local values which that party knows.

The forward direction of~\nameref{thm:mpc-simulation} is proved by induction. The backward direction
requires that we first establish that the distributed semantics enjoys \emph{confluence}. Confluence
says that any executions beginning with the same configuration must eventually arrive at a shared
configuration. For the distributed semantics, this means that for any distributed configuration ⸨D⸩,
if ⸨D ↝⋆ D₁⸩ and ⸨D ↝⋆ D₂⸩ then there must be a ⸨D'⸩ such that ⸨D₁ ↝⋆ D'⸩ and ⸨D₂ ↝⋆ D'⸩. The backward
direction can then be established by appealing to the forward direction, using confluence to rule out the
possibility of rogue executions.

\begin{theorem}[Simulation] \label{thm:mpc-simulation}
  ⸨∀␣⊢ ς⸩, ⸨ς —→⋆ ς'⸩ and ⸨ς'⸩ is terminal ⸨⟺⸩ ⸨ς↯ ↝⋆ ς'↯⸩ and ⸨ς'↯⸩ is terminal.
\end{theorem}

Intuitively,~\nameref{thm:mpc-simulation} tells us that all well-typed, terminating programs
will give us the same answer whether they are executed in the sequential or distributed semantics.
This makes precise what it means for a language to be a \textbf{abstractly sequential}. Programmers
can reason about the behavior of their program without considering all the possible interleaved
executions that a deployment may encounter.
